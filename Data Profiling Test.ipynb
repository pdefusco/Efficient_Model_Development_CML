{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark_df_profiling\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 2.4.0.7.1.2.0-96\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Data Profiling\")\\\n",
    "    .config(\"spark.authenticate\", \"true\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ[\"STORAGE\"])\\\n",
    "    .config(\"spark.hadoop.yarn.resourcemanager.principal\",os.environ[\"HADOOP_USER_NAME\"])\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#.master(\"local[*]\")\\\n",
    "\n",
    "#    .config(\"spark.executor.memory\",\"6g\")\\\n",
    "#    .config(\"spark.executor.cores\",\"3\")\\\n",
    "    \n",
    "# **Note:** \n",
    "# Our file isn't big, so running it in Spark local mode is fine but you can add the following config \n",
    "# if you want to run Spark on the kubernetes cluster \n",
    "# \n",
    "# > .config(\"spark.yarn.access.hadoopFileSystems\",os.getenv['STORAGE'])\\\n",
    "\n",
    "#.config(\"spark.authenticate\", \"true\") \\\n",
    "#    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ['STORAGE'])\n",
    "\n",
    "print(\"Spark Version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cdsw/Efficient_Model_Development_CML\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('inferschema','true').csv('data/olist_customers_dataset.csv', header=True, sep=',', nullValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|                    9790|sao bernardo do c...|            SP|\n",
      "|4e7b3e00288586ebd...|060e732b5b29e8181...|                    1151|           sao paulo|            SP|\n",
      "|b2b6027bc5c5109e5...|259dac757896d24d7...|                    8775|     mogi das cruzes|            SP|\n",
      "|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            campinas|            SP|\n",
      "|879864dab9bc30475...|4c93744516667ad3b...|                   89254|      jaragua do sul|            SC|\n",
      "|fd826e7cf63160e53...|addec96d2e059c80c...|                    4534|           sao paulo|            SP|\n",
      "|5e274e7a0c3809e14...|57b2a98a409812fe9...|                   35182|             timoteo|            MG|\n",
      "|5adf08e34b2e99398...|1175e95fb47ddff9d...|                   81560|            curitiba|            PR|\n",
      "|4b7139f34592b3a31...|9afe194fb833f79e3...|                   30575|      belo horizonte|            MG|\n",
      "|9fb35e4ed6f0a14a4...|2a7745e1ed516b289...|                   39400|       montes claros|            MG|\n",
      "|5aa9e4fdd4dfd2095...|2a46fb94aef5cbeeb...|                   20231|      rio de janeiro|            RJ|\n",
      "|b2d1536598b73a9ab...|918dc87cd72cd9f6e...|                   18682|    lencois paulista|            SP|\n",
      "|eabebad39a88bb6f5...|295c05e81917928d7...|                    5704|           sao paulo|            SP|\n",
      "|1f1c7bf1c9b041b29...|3151a81801c838636...|                   95110|       caxias do sul|            RS|\n",
      "|206f3129c0e4d7d0b...|21f748a16f4e1688a...|                   13412|          piracicaba|            SP|\n",
      "|a7c125a0a07b75146...|5c2991dbd08bbf3cf...|                   22750|      rio de janeiro|            RJ|\n",
      "|c5c61596a3b6bd0ce...|b6e99561fe6f34a55...|                    7124|           guarulhos|            SP|\n",
      "|9b8ce803689b3562d...|7f3a72e8f988c6e73...|                    5416|           sao paulo|            SP|\n",
      "|49d0ea0986edde72d...|3e6fd6b2f0d499456...|                   68485|              pacaja|            PA|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py:401: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  result = results_data.ix[0].copy()\n",
      "/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py:382: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  .limit(50)).toPandas().ix[:,0].value_counts()\n",
      "/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py:344: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  .withColumnRenamed(\"count({c})\".format(c=column), \"freq\")\n",
      "/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py:211: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  .format(col=column, n=x)).toPandas().ix[:,0]\n",
      "/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py:213: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  stats = stats_df.ix[0].copy()\n",
      "/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py:221: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  .agg(df_sum(col(\"delta\"))).toPandas().ix[0,0] / float(current_result[\"count\"]))\n",
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexing.py:961: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  return getattr(section, self.name)[new_key]\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'set_axis_bgcolor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-16f19778bddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfileReport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, bins, sample, corr_reject, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdescription_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_reject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorr_reject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         self.html = to_html(sample,\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(df, bins, corr_reject, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# Do the thing:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mldesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcolum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdescribe_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Compute correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# Do the thing:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mldesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcolum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdescribe_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Compute correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py\u001b[0m in \u001b[0;36mdescribe_1d\u001b[0;34m(df, column, nrows)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescribe_constant_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcolumn_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tinyint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smallint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bigint\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescribe_integer_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcolumn_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"double\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decimal\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescribe_float_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py\u001b[0m in \u001b[0;36mdescribe_integer_1d\u001b[0;34m(df, column, current_result, nrows)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mini_histogram'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/spark_df_profiling/base.py\u001b[0m in \u001b[0;36mmini_histogram\u001b[0;34m(histogram_data)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 facecolor='#337ab7')\n\u001b[1;32m    163\u001b[0m         \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_yaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_bgcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mxticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'set_axis_bgcolor'"
     ]
    }
   ],
   "source": [
    "report = spark_df_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a9346fbaf920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'report' is not defined"
     ]
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
